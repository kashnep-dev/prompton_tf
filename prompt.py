import tempfile

import streamlit as st
from langchain.callbacks.base import BaseCallbackHandler
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter

import embeddings
from custom_prompt_template import CustomPromptTemplate


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


def get_prompt(template_type):
    if template_type == 'get_stock':
        return CustomPromptTemplate.NEWS_TEMPLATE.value
    elif template_type == 'get_finance':
        return CustomPromptTemplate.FINANCE_TEMPLATE.value
    elif template_type == 'get_news':
        return CustomPromptTemplate.STOCK_INFO_TEMPLATE.value
    elif template_type == 'ì¦ê¶Œì•½ê´€ ë¶„ì„':
        return CustomPromptTemplate.DOCUMENT_TEMPLATE.value


def chain_with_api(template_type, model_name, temperature):
    template = get_prompt(template_type)
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", template),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}"),
        ]
    )
    stream_handler = StreamHandler(st.empty())
    llm = ChatOpenAI(
        model=model_name,
        temperature=temperature,
        streaming=True,
        callbacks=[stream_handler]
    )
    chain = prompt | llm
    return chain


def load_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=True) as f:
        f.write(uploaded_file.read())
        f.flush()
        loader = PyPDFLoader(f.name)
        docs = loader.load()
    return docs


def make_prompt_by_file(type, uploaded_file):
    template = get_prompt(type)
    docs = load_pdf(uploaded_file)

    prompt = ChatPromptTemplate.from_template(template)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    splits = text_splitter.split_documents(docs)

    with st.status("íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..", expanded=True) as status:
        st.write("â‘  ì„ë² ë”© ìƒì„±")
        status.update(label="â‘  ì„ë² ë”©ì„ ìƒì„± ì¤‘..ğŸ”¥", state="running")
        embedding = embeddings.embedding_factory()

        st.write("â‘¡ DB ì¸ë±ì‹±")
        status.update(label="â‘¡ DB ì¸ë±ì‹± ìƒì„± ì¤‘..ğŸ”¥", state="running")
        faiss = FAISS.from_documents(splits, embedding["faiss"])

        st.write("â‘¢ Retriever ìƒì„±")
        status.update(label="â‘¢ Retriever ìƒì„± ì¤‘..ğŸ”¥", state="running")
        faiss_retriever = faiss.as_retriever()

        st.write("ì™„ë£Œ âœ…")
        status.update(label="ì™„ë£Œ âœ…", state="complete", expanded=False)
        st.markdown(f'ğŸ’¬ `{uploaded_file.name}`')
        st.markdown("ğŸ””ì°¸ê³ \n\n**ìƒˆë¡œìš´ íŒŒì¼** ë¡œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë ¤ë©´, `ìƒˆë¡œê³ ì¹¨` í›„ ì§„í–‰í•´ ì£¼ì„¸ìš”")
        return faiss_retriever, prompt
