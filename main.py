import time
import os
import tempfile
import prompt as pt
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from langchain.schema import ChatMessage
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from streamlit_feedback import streamlit_feedback
from datetime import datetime, timedelta
from langchain_core.runnables import (
    RunnablePassthrough,
    ConfigurableField,
)
import search
from langchain import hub

load_dotenv()


@st.cache_data(ttl="2h", show_spinner=False)
def get_run_url(run_id):
    time.sleep(1)
    return client.read_run(run_id).url


st.sidebar.title(":books: :blue[  OO ì¦ê¶Œ]")
select_event = st.sidebar.selectbox('How do you want to find data?',
                                    ['ì¢…ëª©ë‰´ìŠ¤ ìš”ì•½', 'ì¬ë¬´ì •ë³´ ìš”ì•½', 'ì£¼ì‹ì •ë³´ ë¶„ì„', 'ì¦ê¶Œì•½ê´€ ë¶„ì„'])

expander = st.sidebar.markdown('## Models and Parameters')
temperature = st.sidebar.slider('temperature Range (0.0 ~ 2.0 )', 0.0, 2.0, 0.2)  # min, max, default
model_name = st.sidebar.selectbox('chose a model name', ['gpt-3.5-turbo', 'gpt-4.0'])

if select_event == 'ì¢…ëª©ë‰´ìŠ¤ ìš”ì•½':
    st.title('Stock New Summary')
    st.markdown("""
                * _Stock News Sentiment Analysis_  
                *  Bing Search, Never News API ë“±ì„ í†µí•œ ì‚¬ì—…ì(ì¢…ëª©)ì— ëŒ€í•œ ë‰´ìŠ¤ ìš”ì•½ì„ í•´ë“œë¦½ë‹ˆë‹¤. 
                """)
elif select_event == 'ì¬ë¬´ì •ë³´ ìš”ì•½':
    st.title('Financial Information Summary')
    st.markdown("""
                """)
elif select_event == 'ì¦ê¶Œì•½ê´€ ë¶„ì„':
    st.title('Document Analysis')
    st.markdown("""
                """)
    uploaded_file = st.sidebar.file_uploader("upload your file", type=['pdf', 'docx', 'pptx'])
    if uploaded_file:
        st.session_state['uploaded_file'] =uploaded_file
    if 'uploaded_file' in st.session_state and 'retriever' not in st.session_state:
        with st.status("íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤ ğŸ§‘â€ğŸ’»ğŸ‘©â€ğŸ’»", expanded=True) as status:
            FLAG_DELETE = True
            if os.name == 'nt':
                FLAG_DELETE = False
            with tempfile.NamedTemporaryFile(delete=FLAG_DELETE) as f:
                f.write(st.session_state["uploaded_file"].read())
                f.flush()
                combined_retriever = pt.make_prompt_by_file(f.name, st, status)
elif select_event == 'ì£¼ì‹ì •ë³´ ë¶„ì„':
    st.title('Stock Information Analysis')
    st.markdown("""
                """)
else:
    st.title('Techical Analysis')
    st.markdown("""
                """)
    context = st.text_input('ì‚¬ì—…ì(ì¢…ëª©)ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”')
    if st.button('ì£¼ì‹ì •ë³´ ë¶„ì„'):
        with st.spinner('[' + context + '] Searching ...'):
            st.text('ì¤€ë¹„ì¤‘ ì…ë‹ˆë‹¤.')

expander = st.sidebar.expander("## About ")
expander.write(""" 
                Introducing Stock Summary and Financial Information Summarization with Generative AI (LLM)

                And Users can easily find the information they need in various documents, including securities terms and conditions.

                """)
########################################################
# If user inputs a new prompt, generate and draw a new response
msgs = StreamlitChatMessageHistory(key="langchain_messages")

reset_history = st.sidebar.button("ì±„íŒ… ì´ˆê¸°í™”")
if len(msgs.messages) == 0 or reset_history:
    msgs.clear()
    msgs.add_ai_message("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    st.session_state["last_run"] = None

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

if user_input := st.chat_input():
    client, run_collector, cfg = pt.configure_run()
    search_result =''
    if select_event == 'ì¢…ëª©ë‰´ìŠ¤ ìš”ì•½':
        search_result = search.search_by_naver_api(user_input.split()[0])
    elif select_event == 'ì¬ë¬´ì •ë³´ ìš”ì•½':
        search_result = search.search_by_dart_api(user_input.split()[0])
    elif select_event == 'ì£¼ì‹ì •ë³´ ë¶„ì„':
        start_date = datetime(datetime.now().year, 1, 1)
        end_date = datetime.now()
        search_result = search.get_yearly_price(search.item_code_by_item_name(user_input.split()[0]))
        search_result = search_result + f"ëŠ” {0}ë¶€í„° {1}ê¹Œì§€ {2}ì˜ ì£¼ì‹ ê°€ê²©ì´ì•¼.".format(start_date, end_date, user_input.split()[0])

    st.session_state.messages.append(ChatMessage(role="user", content=user_input))
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        stream_handler = pt.StreamHandler(st.empty())
        if select_event == 'ì¦ê¶Œì•½ê´€ ë¶„ì„':
            llm = ChatOpenAI(
                model_name="gpt-3.5-turbo",
                temperature=0,
                streaming=True,
                callbacks=[stream_handler],
                api_key=os.getenv["OPENAI_API_KEY"]
            ).configurable_alternatives(
                ConfigurableField(id="llm"),
                default_key="gpt4",
                gpt3=ChatOpenAI(
                    model="gpt-3.5-turbo",
                    temperature=0,
                    streaming=True,
                    callbacks=[stream_handler],
                    api_key=os.getenv["OPENAI_API_KEY"]
                ),
            )

            chain = (
                {
                    "context": st.session_state["retriever"],
                    "question": RunnablePassthrough(),
                }
                | hub.pull("rlm/rag-prompt-mistral")
                | llm
            )
            response = chain.invoke(
                user_input,
            )
        else:
            llm = ChatOpenAI(streaming=True, callbacks=[stream_handler])
            prompt = pt.make_prompt_by_api(select_event)
            chain = prompt | llm
            chain_with_history = RunnableWithMessageHistory(
                chain,
                lambda session_id: msgs,
                input_messages_key="question",
                history_messages_key="history",
            )
            response = chain_with_history.invoke({"question": user_input, "context": search_result}, cfg)
        st.session_state.messages.append(
            ChatMessage(role="assistant", content=response.content)
        )
    st.session_state.last_run = run_collector.traced_runs[0].id

if st.session_state.get("last_run"):
    run_url = get_run_url(st.session_state.last_run)
    st.sidebar.markdown(f"[LangSmith ì¶”ì ğŸ› ï¸]({run_url})")